<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>GLM</title>
    <meta charset="utf-8" />
    <meta name="author" content="Philip Leftwich" />
    <meta name="date" content="2023-07-15" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/my-theme.css" type="text/css" />
    <link rel="stylesheet" href="css/my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide, left, top

# GLM

## Generalized Linear Models Part 2

### Philip Leftwich

&lt;br&gt;



&lt;span style='color:white;'&gt;Slides released under&lt;/span&gt; [CC-BY 2.0](https://creativecommons.org/licenses/by/2.0/)&amp;nbsp;&amp;nbsp;<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M245.83 214.87l-33.22 17.28c-9.43-19.58-25.24-19.93-27.46-19.93-22.13 0-33.22 14.61-33.22 43.84 0 23.57 9.21 43.84 33.22 43.84 14.47 0 24.65-7.09 30.57-21.26l30.55 15.5c-6.17 11.51-25.69 38.98-65.1 38.98-22.6 0-73.96-10.32-73.96-77.05 0-58.69 43-77.06 72.63-77.06 30.72-.01 52.7 11.95 65.99 35.86zm143.05 0l-32.78 17.28c-9.5-19.77-25.72-19.93-27.9-19.93-22.14 0-33.22 14.61-33.22 43.84 0 23.55 9.23 43.84 33.22 43.84 14.45 0 24.65-7.09 30.54-21.26l31 15.5c-2.1 3.75-21.39 38.98-65.09 38.98-22.69 0-73.96-9.87-73.96-77.05 0-58.67 42.97-77.06 72.63-77.06 30.71-.01 52.58 11.95 65.56 35.86zM247.56 8.05C104.74 8.05 0 123.11 0 256.05c0 138.49 113.6 248 247.56 248 129.93 0 248.44-100.87 248.44-248 0-137.87-106.62-248-248.44-248zm.87 450.81c-112.54 0-203.7-93.04-203.7-202.81 0-105.42 85.43-203.27 203.72-203.27 112.53 0 202.82 89.46 202.82 203.26-.01 121.69-99.68 202.82-202.84 202.82z"/></svg><svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M314.9 194.4v101.4h-28.3v120.5h-77.1V295.9h-28.3V194.4c0-4.4 1.6-8.2 4.6-11.3 3.1-3.1 6.9-4.7 11.3-4.7H299c4.1 0 7.8 1.6 11.1 4.7 3.1 3.2 4.8 6.9 4.8 11.3zm-101.5-63.7c0-23.3 11.5-35 34.5-35s34.5 11.7 34.5 35c0 23-11.5 34.5-34.5 34.5s-34.5-11.5-34.5-34.5zM247.6 8C389.4 8 496 118.1 496 256c0 147.1-118.5 248-248.4 248C113.6 504 0 394.5 0 256 0 123.1 104.7 8 247.6 8zm.8 44.7C130.2 52.7 44.7 150.6 44.7 256c0 109.8 91.2 202.8 203.7 202.8 103.2 0 202.8-81.1 202.8-202.8.1-113.8-90.2-203.3-202.8-203.3z"/></svg> ]   

&lt;div style = "position: absolute;top: 0px;right: 0px;"&gt;&lt;img src="images/logo.png" alt="The hex logo for plumbertableau package" width="500"&gt;&lt;/img&gt;&lt;/div&gt;

---

layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;Philip Leftwich - Physalia Courses&lt;/span&gt;&lt;/div&gt;

---
class: center

## GLM error distributions



&lt;img src="images/prob_distributions2.png" title="There are many other types of probability distributions beyond the Normal (Gaussian), these are related to each other - and for example a sampling distribution may follow a Poisson error distribution, but become a normal distribution if sample sizes are large enough" alt="There are many other types of probability distributions beyond the Normal (Gaussian), these are related to each other - and for example a sampling distribution may follow a Poisson error distribution, but become a normal distribution if sample sizes are large enough" width="80%" /&gt;


---

## GLM error distributions

&lt;div class="figure"&gt;
&lt;img src="images/glm equation.png" alt="GLM models the estimate of the mean as a separate equation to estimates of the error distribution" width="80%" /&gt;
&lt;p class="caption"&gt;GLM models the estimate of the mean as a separate equation to estimates of the error distribution&lt;/p&gt;
&lt;/div&gt;

---

## GLM error distributions

|Family|Canonical link|Other links|Common uses|
|-----|-----|-----|-----|
|gaussian|identity| sqrt, log| Continuous data with a normal distribution|
|binomial|logit|probit, cloglog| Binary/Bernoulli data(0,1), proportional/binomial data|
|poisson|log|identity, sqrt| Count/rate data (integers)|
|Gamma|inverse|identity, log|Continuous data where variance increases with the value of the mean|
|quasibinomial|logit| | overdispersed binomial|
|quasipoisson|log| | overdispersed poisson|


---

## GLM workflow

**1.** Exploratory data analysis

**2.** Choose a suitable error term that fits your data type

**3.** Choose a suitable link function - or stick with the canonical link

**4.** Fit the model: check assumptions

**5.** Simplify and refine model if necessary

**6.** Check the final model fit



---

## Count/rate data

.pull-left[

* Integer values (whole numbers) 

* We do not expect count/rate data to follow a gaussian distribution.

* Zero-bounded

Instead this is is often better resembles a Poisson distribution:

* Poisson distributions can be modeled with a single parameter `\(\lambda\)`

* When mean and variance both equal `\(\lambda\)` we expect variance to rise at the same rate as mean

]

.pull-right[

&lt;img src="images/lambda.png" width="80%" /&gt;

]

---

## Visualisation

Imagine a road that is often very busy - it has a mean count/rate of through traffic of 200 cars/hour

  - The variance is likely to be high here : compare rush hour traffic to the middle of the night

Imagine another road that is usually very quiet - it has a mean count/rate of only 2 cars per hour. 

  - variance here is low, often 0 cars, sometimes perhaps as high as 4/5 per hour. 

  
    
**Q. Can you think of any other types of data that might fit a Poisson distribution?**


---
class: center, middle

## Poisson distribution

$$
(log)\lambda=\beta0+\beta1x
$$

$$
Y\text{~}~Pois(\lambda)
$$

```
glm(outcome ~ predictors, data = df, family = poisson(link = "log"))

```
---

### Why log-link?



&lt;img src="images/poisson_both.png" width="90%" /&gt;



---

## Examples

.pull-left[


```r
library(datasets)

head(warpbreaks)
```

```
##   breaks wool tension
## 1     26    A       L
## 2     30    A       L
## 3     54    A       L
## 4     25    A       L
## 5     70    A       L
## 6     52    A       L
```

]

.pull-right[


```r
hist(warpbreaks$breaks)
```

&lt;img src="Generalized-Linear-Models-Part-2_files/figure-html/unnamed-chunk-6-1.png" width="120%" /&gt;

]

---

## Interpret the Poisson GLM


```r
poisson_model &lt;- glm(breaks ~ wool + tension, data = warpbreaks, family = poisson(link = "log"))

summary(poisson_model)
```

```
## 
## Call:
## glm(formula = breaks ~ wool + tension, family = poisson(link = "log"), 
##     data = warpbreaks)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.6871  -1.6503  -0.4269   1.1902   4.2616  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.69196    0.04541  81.302  &lt; 2e-16 ***
## woolB       -0.20599    0.05157  -3.994 6.49e-05 ***
## tensionM    -0.32132    0.06027  -5.332 9.73e-08 ***
## tensionH    -0.51849    0.06396  -8.107 5.21e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 297.37  on 53  degrees of freedom
## Residual deviance: 210.39  on 50  degrees of freedom
## AIC: 493.06
## 
## Number of Fisher Scoring iterations: 4
```

---


```r
check_model(poisson_model)
```

&lt;img src="Generalized-Linear-Models-Part-2_files/figure-html/unnamed-chunk-8-1.png" width="70%" /&gt;

---


```r
int_poisson_model &lt;- glm(breaks ~ wool * tension, data = warpbreaks, family = poisson(link = "log"))

summary(int_poisson_model)
```

```
## 
## Call:
## glm(formula = breaks ~ wool * tension, family = poisson(link = "log"), 
##     data = warpbreaks)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.3383  -1.4844  -0.1291   1.1725   3.5153  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     3.79674    0.04994  76.030  &lt; 2e-16 ***
## woolB          -0.45663    0.08019  -5.694 1.24e-08 ***
## tensionM       -0.61868    0.08440  -7.330 2.30e-13 ***
## tensionH       -0.59580    0.08378  -7.112 1.15e-12 ***
## woolB:tensionM  0.63818    0.12215   5.224 1.75e-07 ***
## woolB:tensionH  0.18836    0.12990   1.450    0.147    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 297.37  on 53  degrees of freedom
## Residual deviance: 182.31  on 48  degrees of freedom
## AIC: 468.97
## 
## Number of Fisher Scoring iterations: 4
```

---


```r
check_model(int_poisson_model)
```

&lt;img src="Generalized-Linear-Models-Part-2_files/figure-html/unnamed-chunk-10-1.png" width="70%" /&gt;

---

## Use deviance for hypothesis testing


These follow the simpler `\(\chi^2\)` distribution

$$
\chi^2= Full~model~deviance~–~ Reduced~model~deviance
$$



```r
drop1(int_poisson_model, 
      test = "Chi")
```

```
## Single term deletions
## 
## Model:
## breaks ~ wool * tension
##              Df Deviance    AIC    LRT  Pr(&gt;Chi)    
## &lt;none&gt;            182.31 468.97                     
## wool:tension  2   210.39 493.06 28.087 7.962e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

## Overdispersion

When the residual deviance/residual df is &gt;1 we have indications of Overdispersion.
&gt; Note dispersion parameter in summary of model

* Variance is greater than expected from a Poisson distribution

* Confidence intervals and hypothesis testing will be affected

* Increased risk of Type 1 errors


```
## # A tibble: 1 x 8
##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs
##           &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;
## 1          297.      53  -228.  469.  481.     182.          48    54
```

---




```r
broom::tidy(quasi_poisson_model)
```

```
## # A tibble: 6 x 5
##   term           estimate std.error statistic  p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       3.80     0.0969    39.2   4.19e-38
## 2 woolB            -0.457    0.156     -2.94  5.10e- 3
## 3 tensionM         -0.619    0.164     -3.78  4.36e- 4
## 4 tensionH         -0.596    0.163     -3.67  6.16e- 4
## 5 woolB:tensionM    0.638    0.237      2.69  9.73e- 3
## 6 woolB:tensionH    0.188    0.252      0.747 4.58e- 1
```




```r
broom::tidy(int_poisson_model)
```

```
## # A tibble: 6 x 5
##   term           estimate std.error statistic  p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       3.80     0.0499     76.0  0       
## 2 woolB            -0.457    0.0802     -5.69 1.24e- 8
## 3 tensionM         -0.619    0.0844     -7.33 2.30e-13
## 4 tensionH         -0.596    0.0838     -7.11 1.15e-12
## 5 woolB:tensionM    0.638    0.122       5.22 1.75e- 7
## 6 woolB:tensionH    0.188    0.130       1.45 1.47e- 1
```

---

### Hypothesis testing on Quasi-likelihood models

* We have now reintroduced a separate term modelling variance

* Quasilikelihood fitting is therefore more accurate under an F distribution


```r
drop1(quasi_poisson_model, test = "F")
```

```
## Single term deletions
## 
## Model:
## breaks ~ wool * tension
##              Df Deviance F value Pr(&gt;F)  
## &lt;none&gt;            182.31                 
## wool:tension  2   210.39  3.6975 0.0321 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---


```r
library(emmeans)

means_summary&lt;- 
  emmeans(quasi_poisson_model, 
        specs = "tension", "wool",
*       type = "response")

means_summary
```

```
## wool = A:
##  tension rate   SE  df asymp.LCL asymp.UCL
##  L       44.6 4.32 Inf      36.8      53.9
##  M       24.0 3.17 Inf      18.5      31.1
##  H       24.6 3.20 Inf      19.0      31.7
## 
## wool = B:
##  tension rate   SE  df asymp.LCL asymp.UCL
##  L       28.2 3.44 Inf      22.2      35.8
##  M       28.8 3.47 Inf      22.7      36.4
##  H       18.8 2.80 Inf      14.0      25.2
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale
```

---

.left-code[


```r
means_summary %&gt;% 
  as_tibble() %&gt;% 
  ggplot(aes(x = tension,
             y = rate,
             colour = wool))+
  geom_pointrange(aes(ymin = asymp.LCL,
                      ymax = asymp.UCL),
                  position = position_dodge(
                    width = 0.2))
```

]

.right-plot[

&lt;img src="Generalized-Linear-Models-Part-2_files/figure-html/unnamed-chunk-19-1.png" width="120%" /&gt;

]

---

## Logistic regression

* Binary repsonse variable

* Explanatory variables - continuous or categorical

* Link: log-odds

**Why**

An OLS allows for unconstrained outcomes

But we know our predictions *should* be constrained to between 0 and 1

---

## Examples

* Data with a Binomial/Bernoulli distribution

* Anything with a two-level categorical *response*

    - Dead/alive
    - Pass/fail

.pull-right[

&lt;img src="images/binomial_joke.png" width="100%" /&gt;

]
---
class:center, middle

## The logit (logistic) model

$$
logit(p)=log({p\over1-p})=\beta0+\beta1x
$$

Exponentiated:

$$
p={e^{\beta0+\beta1x}\over{1+e^{\beta0+\beta1x}}}
$$

---

## Why work with log-odds?

.pull-left[

&lt;img src="images/probability.png" width="90%" /&gt;


$$
p={e^{\beta0+\beta1x}\over{1+e^{\beta0+\beta1x}}}
$$



]


.pull-right[

&lt;img src="images/log_odds.png" width="90%" /&gt;

$$
logit(p)=log({p\over1-p})=\beta0+\beta1x
$$

]

---

## Presence/absence data

&lt;img src="images/mayfly.png" width="70%" /&gt;

We are interested in the abundance of mayflies in a stream. Because mayflies are sensitive to metal pollution, I might be interested in looking at the presence/absence of mayflies in a stream relative to a pollution gradient. Here the pollution gradient is measured in **Cumulative Criterion Units** 

```
glm(occupancy ~ CCU, data = mayfly, family = binomial, link = "logit")

```


---


## Summary

```
Call:
glm(formula = occupancy ~ CCU, data = mayfly, 
family = binomial(link=“logit”))


Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)    5.102      2.369   2.154   0.0313 *
## CCU           -3.051      1.211  -2.520   0.0117 *

##     Null deviance: 34.795  on 29  degrees of freedom
## Residual deviance: 12.649  on 28  degrees of freedom
## AIC: 16.649

```

Negative coefficients: log-odds of higher ranks less than log-odds of lower ranks (cumulatively)

Positive coefficients: log odds of higher ranks greater than odds of lower ranks

No matter how you slice it, interpreting coefficients w.r.t. log-odds or odds is challenging. It’s much easier to interpret estimated probabilities.


---

## Model checks

.left-code[

* Similar plots are produced.

* Overdispersion here can be picked up in the Binned Residuals plot

* Normality of Residuals plot not very helpful here

]

.right-plot[

&lt;img src="images/Rplot.png" width="100%" /&gt;

]


---

## Probabilities

```
broom::augment(binary_model, type.predict="response")

```

&lt;img src="images/fitted_mayfly.png" width="80%" /&gt;


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "dracula",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
