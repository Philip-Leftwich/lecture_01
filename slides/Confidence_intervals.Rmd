---
title: "Uncertainty"
subtitle: "Standard Error and Confidence Intervals"
author: "Philip Leftwich"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "css/my-theme.css", "css/my-fonts.css"]
    seal: false
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: dracula
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        eval = TRUE, warning = FALSE, message = FALSE)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.retina = 3, fig.asp = 0.8, fig.width = 7, out.width = "120%")

library(tidyverse)
library(gt)
library(gtExtras)
library(DiagrammeR)
library(palmerpenguins)

```


class: title-slide, left, top

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

<br>



<span style='color:white;'>Slides released under</span> [CC-BY 2.0](https://creativecommons.org/licenses/by/2.0/)&nbsp;&nbsp;`r fontawesome::fa("creative-commons", "white")``r fontawesome::fa("creative-commons-by", "white")` ]   

<div style = "position: absolute;top: 0px;right: 0px;"><img src="images/physalia.png" alt="The hex logo for plumbertableau package" width="500"></img></div>

---

layout: true

<div class="my-footer"><span>Philip Leftwich - Physalia Courses</span></div>

---

## Sampling distribution

If we had a bunch of samples, and found their *probability distribution*(e.g. normal, mean and sd etc), then we would be able to predict the **sampling distribution**

```{r, eval = F}
# function to generate samples with a normal distribution
rnorm(n = 100, mean = 10, sd  = 1)

```

--

When looking at standard deviation we looked at whether our model (the mean) was a good fit for *our data*.

But we can move beyond this to look at whether it is a good fit for *our population*.

---

**Standard Deviation (SD):**

The standard deviation is a measure of the spread or dispersion of data points within a sample or population. It quantifies the average amount by which individual data points deviate from the mean. A higher standard deviation indicates greater variability in the data.

--

**Standard Error (SE):**

The standard error is a measure of the uncertainty or precision associated with *estimating a population parameter*, typically the mean. It quantifies the average amount of variation between sample means if multiple samples are taken from the same population. The standard error is closely related to the standard deviation, but it is divided by the square root of the sample size

---

## Standard Error

The **standard deviation of the sampling distribution of means** is the **standard error**. It is calculated by dividing the standard deviation by the square root of the sample size:

$$
SE = {\sigma\over\sqrt n}
$$

If we took hundreds of samples and independently calculated the mean of each sample, we would get a nice, even, sampling distribution - which if you averaged it would get back to the population mean.


---

.right-plot[

```{r, echo = FALSE, warning = FALSE}

library(DiagrammeR)

grViz("digraph {
  
graph[layout = dot]

a[label = 'Population mean = 3']
b[label = 'Sample mean = 3']
c[label = 'Sample mean = 4']
d[label = 'Sample mean = 5']
e[label = 'Sample mean = 1']
f[label = 'Sample mean = 2']
g[label = 'Sample mean = 3']
h[label = 'Sample mean = 3']
i[label = 'Sample mean = 4']
j[label = 'Sample mean = 2']

a -> b
a -> c
a -> d
a -> e
a -> f
a -> g
a -> h
a -> i
a -> j



}")


```

]

.left-code[


```{r, echo = FALSE, warning = FALSE}

sample <- c(3,4,5,1,2,3,3,4,2)
df1 <- data.frame(x = sample)

ggplot(df1, aes(x = x)) +
    geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  theme_classic()

```

]

---

## Standard Error

In reality we cannot collect hundreds of sample means, so we rely on this approximation of the standard error. 

Luckily for us it has been demonstrated that when the number of observations in a sample gets large (usually defined as *at least 30*), the sampling distribution has an approximate normal distribution.

--

This is known as **central limit theorem**

$$\sigma_{\bar{x}} = \frac{s}{\sqrt{n}}$$
Interestingly (and handily for us) central limit theorem applies - *even if the population distribution doesn't have a normal distribution*

- When the sample size is small we have to rely on a different (*t*) distribution, more on this later.

---

## Standard Error

How much does your **sample statistic** differ from the population parameter *aka* how accurate is your mean?

**Q. What happens to SE as sample size increases?**



$$
SE = {s\over\sqrt n}
$$


---

## Large enough sample size

**Q. What is a large enough sample size?**

--

You will often hear *n* $\geq$ *30* as large enough to claim means are normally distributed according to CLT

--

This depends on how far removed the underlying shape of a population *is* from a normal distribution

* If the population distribution is symmetrical, it could be as small as 15.

* If the population is sufficiently skewed it could be as high as 100. 

--

Also see **bootstrapping**

---
class: center, middle, inverse

## Confidence Intervals

---

## Common misconception

A calculated boundary within which we think the true value of the mean will sit: 

"There is a 95% chance that the **true** population mean is within this range"

--

The mean either *is* or *is not* within this range

--

"If we repeat this experiment many times, 95% of our computed confidence intervals would contain the *true mean*"

Frame the probability around the intervals not the population parameter.

```{r, echo = FALSE, out.width="50%", fig.alt = " Confidence Intervals"}

knitr::include_graphics("images/CI-experiments.png")
```

---

For that specific experiment, either the ring landed on the target or it didn't, either the confidence interval based on our experimental data has the true value in it or doesn't.

There's a 95% chance.

```{r, echo = FALSE, out.width="70%", fig.alt = "Ring toss"}
knitr::include_graphics("images/ring-toss.jpg")
```

---

## Confidence levels

Can be constructed by YOU:

Do you want to know the range within which you will capture the mean...

* 66% of the time = 1 * S.E.

--

* 95% = 1.96 * S.E. 

--

* 99% = 2.58 * S.E. 

--

The Confidence level (C) is the inverse of the significance level.

$$
\huge\alpha = 1 - C
$$
---

Example: We calculate a confidence interval for mean flipper lengths in male and female penguins (n = 165, 168), finding a 95% confidence interval of [203.4 - 206.6]mm for males and [195.7 - 198.3]mm for females   

**What does that mean?**

If we took many samples and found the confidence interval for each, then we’d expect 95% of the calculated confidence intervals to contain the population parameters. So...there’s a pretty good chance that a calculated confidence interval contains the population parameter...increasing with increasing confidence level.   

.left-code[


```{r, eval = F, out.width = "60%"}
flipper <- penguins %>% 
  group_by(sex) %>% 
  drop_na(sex) %>% 
  summarise(mean = mean(flipper_length_mm, na.rm = T),
            sd = sd(flipper_length_mm, na.rm = T),
            n = n(),
            se = sd/sqrt(nrow(.)),
            ci = se*1.96)

ggplot(data = flipper, aes(x = mean, y = sex, group = sex))+
  geom_pointrange(aes(xmin = mean-ci, xmax = mean+ci))+
  labs(x = "Flipper length (mm)", y = "")+
  theme_classic()+
  coord_cartesian(xlim = c(195,210))

```
]

.right-plot[

```{r, echo = F}
flipper <- penguins %>% 
  group_by(sex) %>% 
  drop_na(sex) %>% 
  summarise(mean = mean(flipper_length_mm, na.rm = T),
            sd = sd(flipper_length_mm, na.rm = T),
            n = n(),
            se = sd/sqrt(nrow(.)),
            ci = se*1.96)

ggplot(data = flipper, aes(x = mean, y = sex, group = sex))+
  geom_pointrange(aes(xmin = mean-ci, xmax = mean+ci))+
  labs(x = "Flipper length (mm)", y = "")+
  theme_classic()+
  coord_cartesian(xlim = c(195,210))

```

]


---

```{r, echo = FALSE, out.width="90%", fig.alt = " Confidence Intervals"}

knitr::include_graphics("images/confidence-interval-area.png")
```

---

## Calculating a Confidence interval

This requires us to **know** the population variance (standard deviation)


$$
\bigg[\overline \mu - z_{a\over2} * {s\over {\sqrt n}}
$$


$$
\overline \mu + z_{a\over2} *{s\over {\sqrt n}}\bigg]
$$

--


**Q. How likely is this?**


---

## Z distribution

So far we have used the z-distribution. This is OK for normally distributed data, where we have population $\mu$ and $s$. 

When we don't *know* the population mean and SD

--

Then we cannot be sure of our uncertainty

--

Three choices: 

1. Assume we have a big enough sample size and CLT fixes this

2. Use another type of sampling distribution (e.g. *t*-distribution)

3. Bootstrap our sampling distribution

---

## Bootstrapping

```{r, echo = FALSE, out.width="120%", fig.alt = "With as few as 10 data points in our random sample, it is possible to do  bootstapped resampling (minimum ~ 1000 samples). We can use this to find the sampling distribution of the mean, se and 95% CI "}

knitr::include_graphics("images/bootstrapping.png")
```

---

## Bootstrapping

.left-code[

```{r, eval = TRUE}
library(boot)

flipper <- penguins %>% 
  filter(sex == "male") %>% 
  pull(flipper_length_mm)

mean_fun <- function(x,i){
  mean(x[i])
}

boot_1000 <- boot( #<<
  data = flipper,#<<
  statistic = mean_fun,#<<
  R = 1000#<<
)


```
]


.right-plot[

```{r}
boot_1000
```

]

---

.left-code[

```{r, eval = FALSE}
ggplot()+
  aes(boot_1000$t)+
  geom_histogram()

```

]


.right-plot[

```{r, echo = FALSE}
ggplot()+
  aes(boot_1000$t)+
  geom_histogram()

```

]

---

## Bootstrap Confidence Intervals

```{r}
boot.ci(boot_1000,
        conf = 0.95,
        type = "perc")

```

---
class: center, middle, inverse

# Next time
# T-distributions

