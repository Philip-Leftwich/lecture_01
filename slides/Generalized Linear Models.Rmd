---
title: "GLM"
subtitle: "Intro to Generalized Linear Models"
author: "Philip Leftwich"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "css/my-theme.css", "css/my-fonts.css"]
    seal: false
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: dracula
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        eval = TRUE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.retina = 3, fig.asp = 0.8, fig.width = 7, out.width = "120%")

library(tidyverse)
library(gt)
library(gtExtras)
library(rstatix)
library(palmerpenguins)
library(here)
library(performance)

```


class: title-slide, left, top

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

<br>



<span style='color:white;'>Slides released under</span> [CC-BY 2.0](https://creativecommons.org/licenses/by/2.0/)&nbsp;&nbsp;`r fontawesome::fa("creative-commons", "white")``r fontawesome::fa("creative-commons-by", "white")` ]   

<div style = "position: absolute;top: 0px;right: 0px;"><img src="images/logo.png" alt="The hex logo for plumbertableau package" width="500"></img></div>

---

layout: true

<div class="my-footer"><span>Philip Leftwich - Physalia Courses</span></div>

---

## Generalized Linear Models

class: center, middle

.left-code[

* **Generalized** Linear Models

* Separating the mean from the error distribution

* Maximum Likelihood

* Modelling non-linear data

]

.right-plot[

```{r, echo = FALSE, out.width="110%", fig.alt = "Linear regression is everywhere"}
knitr::include_graphics("images/linear_regression_everywhere.jpg")
```
]


---

## Recap OLS

```{r, echo = FALSE, out.width="70%", fig.alt = "OLS fits a line to produce the smallest amount of SSE"}

knitr::include_graphics("images/Sum of squares.png")

```

---
## Recap OLS

$$
\LARGE{\hat{y_i} = a + bx}
$$
where:

$\hat{y_i}$ = value estimated by the regression

$a$ is the intercept (value of y when x = 0)

$b$ is the slope of the regression line

$x$ is the value of the explanatory variable


---

## Recap OLS

$$
\LARGE{y_i = a + bx+\epsilon}
$$

### Where:

$y_i$ is the **observed** value of the response variable

$a$ is the intercept (value of y when x = 0)

$b$ is the slope of the regression line

$x$ is the value of the explanatory variable

$\epsilon$ is the value of the residual error

---

## OLS assumptions


.right-plot[
```{r, echo = FALSE, out.width="100%", fig.alt = "Outline of a hypothetical regression"}

knitr::include_graphics("images/ols_assumption.png")

```
]

--

.left-code[

* Linear relationship

* Homogeneity of variance

* Normal distribution of residual variance

* Independent predictors


]

---

## Generalized Linear Models

A **Generalized** Linear Model is more flexible than a standard linear model because it allows us to separate the **error distribution** from the estimate of the **mean**

There are three components in a generalized linear model:

1. Linear Predictor – the equation of the slope and intercept

2. Link function – literally links the linear predictor to the mean (like an internal transformation)

3. Error distribution – so far we have always assumed Gaussian (Normal distribution) but can be others e.g Poisson, Binomial etc. 


---

## Generalized Linear Models

.pull-left[

**OLS Linear Model**

$$
\LARGE{y_i = \beta0 + \beta1x+\epsilon}
$$

]

--

.pull-right[

**Generalized Linear Model**

Linear predictor of the mean - with a link function to specify transformation

$$
\LARGE{(Identity)\mu = \beta0 + \beta1x}
$$
The error distribution, here indicated as normal distribution of variance

$$
\LARGE{y_i ~ N(\mu, \sigma^2) }
$$
]


---

```{r, echo = FALSE}
fruitfly <- readRDS(here::here("data", "fruitfly.rds"))

```


```{r}
lm(longevity ~ type + thorax, data = fruitfly) %>% 
  broom::tidy()
```


```{r}
glm(longevity ~ type + thorax, data = fruitfly, family= gaussian(link = "identity")) %>% 
  broom::tidy()
```



---

## What's the difference between transforming the data and using a link function?

```{r, echo = FALSE}

set.seed(55)
n <- 50
x <-  seq(0, 1, length.out = n) #pollution levels

gamma <- -3.2 #effect of polluted reefs
alpha <- 4 #intercept = mean at 'zero' pollution
yexp <- alpha*exp(gamma*x)

yobs_pois <- rpois(n, yexp)
yobsplus <- yobs_pois+0.1

```

```{r, out.width = "50%"}
lsmodel1 <- lm(yobsplus ~ x)
MASS::boxcox(lsmodel1)

```

---

```{r}
model1 <- glm(yobsplus ~ x, family = gaussian(link = "log"))
broom::tidy(model1)
```


```{r}
model2 <- glm(log(yobsplus) ~ x, family = gaussian(link = "identity"))
broom::tidy(model2)
```

---
.pull-left[


```{r, out.width = "110%"}
model1 %>% 
  check_model()

```

]

.pull-right[

```{r,out.width = "110%"}
model2 %>% 
  check_model()

```

]

---

## Types of GLM

| Family             | Canonical mean             | Variance Function (V)      |
|--------------------|----------------------------|----------------------------|
| Gaussian           | μ                          | V = σ²                      |
| Binomial           | logit(μ)                   | μ(1 - μ)/n                  |
| Poisson            | exp(μ)                     | μ                          |
| Gamma              | μ⁻¹                        |  μ²ϕ                        |

- Gaussian: This family assumes a continuous, normally distributed response variable
- Binomial: The binomial family is appropriate for binary response variables representing two mutually exclusive outcomes, such as success/failure or yes/no
- Poisson: The Poisson family is suited for count data that follows a Poisson distribution, such as the number of events occurring within a fixed interval
- Gamma: The gamma family is commonly used for modeling continuous, positive response variables with a skewed distribution and right-skewed tail, such as time or duration data. It allows for modeling heteroscedasticity

---

## Maximum Likelihood

Maximum likelihood is a method that determined the values for the parameters of a model.

The values are chosen such that they *maximise* the *likelihood* that the process described by the model produces the observed data. 


$$
\LARGE{L(Y|\theta)}
$$

where:

$L$ = Likelihood

$\theta$ = unknown parameter

---

## Maximum Likelihood

As an example let's assume we are fitting our data using a model describing a Gaussian (normal) distribution.

$$
\LARGE{L(data;\mu,\sigma)}
$$
A Gaussian distribution has **two** parameters. The mean $\mu$ and the standard deviation $\sigma$. Different values of these two parameters will result in different regression lines/fits to the data. 

They will also produce different *likelihood curves*. 

We want to know which combinations of mean and standard deviation produce the curve where we are *most likely* to observe all of the data points. 

---

## Maximum Likelihood

.right-plot[

```{r, echo = FALSE, out.width="100%", fig.alt = "The 10 data points and possible Gaussian distributions from which the data were drawn. f1 is normally distributed with mean 10 and variance 2.25 (variance is equal to the square of the standard deviation), this is also denoted f1 ∼ N (10, 2.25). f2 ∼ N (10, 9), f3 ∼ N (10, 0.25) and f4 ∼ N (8, 2.25). The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data."}

knitr::include_graphics("images/likelihood_curve.png")

```
]

.left-code[

When the model can be explained by least squares, the outcome of the maximum likelihood will be identical. Both are designed to minimise error. 

Maximum likelihoods can solve a wider range of probability distributions, but are difficult to solve by hand in the same way as OLS, as they use these iterative methods. 
]
---


```{r, out.width = "80%"}
fly_glm <- glm(longevity ~ type + thorax, data = fruitfly, family= gaussian(link = "identity")) 

summary(fly_glm)

```

The GLM summary output, while mostly identical to the LM as the following differences:

* Deviance

* Dispersion, AIC & Fisher Scoring 

---

```{r}

fly_lm <- lm(longevity ~ type + thorax, data = fruitfly) 

summary(fly_lm)

```


---

## Deviance

A generalized linear model can be characterised in terms of two types of deviance:

1. The **null deviance**, which is a measure of the overall variability in the response variable (equivalent to Sum of Squares Total):

$\Large{Dev_{null}=2(LnL_{saturated}-LnL_{null})}$

2. The **residual deviance**, which is a measure of the variability in the response variable that remains unexplained by the proposed model (equivalent to Sum of Squares Errors):

$\Large{Dev_{residual}=2(LnL_{saturated}-LnL_{proposed})}$

In the previous example we had:

**Null deviance**: 38253 k-1 = 3

**Residual deviance**: 15211 with df(N-k) = 121



---

## F distribution

.right-plot[
```{r, echo = FALSE, out.width="60%", fig.alt = "The Saturated Model: is a model that assumes each data point has its own parameters, which means you have n parameters to estimate. The Proposed Model: assumes you can explain your data points with k parameters plus an intercept term, so you have k+1 parameters. The Null Model: assumes the exact “opposite”, it assumes one parameter for all of the data points, which means you only estimate 1 parameter."}

knitr::include_graphics("images/deviance_explained.png")

```

]

.left-code[

**Null deviance**: 38253 with k-1 = 3

**Residual deviance**: 15211 with residual df = 121

We can compare these values: 

$\Delta Deviance=Null~deviance–Residual~deviance$

$\Delta Deviance$ = 38253-15211 = 23042

F = $\frac{\Delta Deviance/k-1}{Residual~deviance/N-k}$

F = $\frac{23042/3}{15211/121} = 61.09$

]
---

## Model simplification

Changes in deviance also follow the *F statistic* with *k* numerator and *n-k* denominator degrees of d.

.pull-left[
```{r}
drop1(fly_glm, test = "F")

```
]

.pull-right[

```{r}
drop1(fly_glm, test = "Chi")

```
]

The models are clearly the same, *but* F-tests for models where we estimate variance independently of the mean are a better choice for hypothesis testing. We will use chi-square distributions when using alternative error distributions e.g. Poisson & Binomial.

---

```{r, echo = FALSE, out.width = "80%"}
# Set the seed for reproducibility
set.seed(123)

# Parameters
k <- 1  # Number of predictors
n <- 10000  # Number of observations

# Generate chi-squared distribution
chi_squared <- rchisq(n, df = k)

# Generate F distributions with different residual degrees of freedom
residual_df <- c(5, 10, 50)  # Different residual df
f_distributions <- vector("list", length(residual_df))

for (i in 1:length(residual_df)) {
    f_distributions[[i]] <- rf(n, df1 = k, df2 = residual_df[i])
}

# Combine the distributions into one data frame
data <- data.frame(Values = c(chi_squared, unlist(f_distributions)),
                   Distribution = rep(c("Chi-squared", "F"), times = c(n, n * length(residual_df))),
                   Residual_df = rep(c("k = 1", paste("residual df =", residual_df)), each = n))
data$Residual_df <- factor(data$Residual_df, levels = c("k = 1", "residual df = 50", "residual df = 10", "residual df = 5"))
# Plot the distributions
library(ggplot2)

ggplot(data, aes(x = Values, fill = Residual_df)) +
    geom_density(binwidth = 0.5, alpha = 0.2, position = "identity") +
    facet_wrap(~ Distribution + Residual_df, nrow = 2) +
    labs(x = "Value", y = "Density", fill = "Residual df") +
    ggtitle("Comparison of Chi-squared and F Distributions") +
    theme_minimal()+coord_cartesian(xlim = c(0,10))

```


---

## AIC: Akaike Information Criterion

A quantitative way to compare regression models.

**Unlike likelihood ratio test it CAN be used on non-nested models**

Takes into account how well the model predicts the data, while penalising it for increasing complexity

We want a descriptive model that balances prediction and complexiy, and is given by the model with the **lowest AIC value**

$\Large{AIC=2k-2Ln(L)}$

where:

$k$ is the number of parameters

$L$ is maximum likelihood

>If a model is more than 2 AIC units lower than another it can be considered a better model


---

```{r}
summary(fly_glm)


fly_glm_sqrt <- glm(longevity ~ type + thorax, data = fruitfly, family= gaussian(link = "sqrt")) 

AIC(fly_glm, fly_glm_sqrt)

```

---

```{r}

drop1(fly_glm, test = "F")

```

---

## Other outputs

###Dispersion

Dispersion (variability/scatter/spread) simply indicates whether a distribution is wide or narrow. The GLM function can use a dispersion parameter to model the variability.

###Fisher score

A verbose output of how many iterations were required to resolve the model. A high number indicates the model may be having trouble converging (could be mis-specified)

