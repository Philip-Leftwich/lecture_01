<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>GLM</title>
    <meta charset="utf-8" />
    <meta name="author" content="Philip Leftwich" />
    <meta name="date" content="2022-09-16" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/my-theme.css" type="text/css" />
    <link rel="stylesheet" href="css/my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide, left, top

# GLM

## Intro to Generalized Linear Models

### Philip Leftwich

&lt;br&gt;



&lt;span style='color:white;'&gt;Slides released under&lt;/span&gt; [CC-BY 2.0](https://creativecommons.org/licenses/by/2.0/)&amp;nbsp;&amp;nbsp;<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M245.83 214.87l-33.22 17.28c-9.43-19.58-25.24-19.93-27.46-19.93-22.13 0-33.22 14.61-33.22 43.84 0 23.57 9.21 43.84 33.22 43.84 14.47 0 24.65-7.09 30.57-21.26l30.55 15.5c-6.17 11.51-25.69 38.98-65.1 38.98-22.6 0-73.96-10.32-73.96-77.05 0-58.69 43-77.06 72.63-77.06 30.72-.01 52.7 11.95 65.99 35.86zm143.05 0l-32.78 17.28c-9.5-19.77-25.72-19.93-27.9-19.93-22.14 0-33.22 14.61-33.22 43.84 0 23.55 9.23 43.84 33.22 43.84 14.45 0 24.65-7.09 30.54-21.26l31 15.5c-2.1 3.75-21.39 38.98-65.09 38.98-22.69 0-73.96-9.87-73.96-77.05 0-58.67 42.97-77.06 72.63-77.06 30.71-.01 52.58 11.95 65.56 35.86zM247.56 8.05C104.74 8.05 0 123.11 0 256.05c0 138.49 113.6 248 247.56 248 129.93 0 248.44-100.87 248.44-248 0-137.87-106.62-248-248.44-248zm.87 450.81c-112.54 0-203.7-93.04-203.7-202.81 0-105.42 85.43-203.27 203.72-203.27 112.53 0 202.82 89.46 202.82 203.26-.01 121.69-99.68 202.82-202.84 202.82z"/></svg><svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:white;overflow:visible;position:relative;"><path d="M314.9 194.4v101.4h-28.3v120.5h-77.1V295.9h-28.3V194.4c0-4.4 1.6-8.2 4.6-11.3 3.1-3.1 6.9-4.7 11.3-4.7H299c4.1 0 7.8 1.6 11.1 4.7 3.1 3.2 4.8 6.9 4.8 11.3zm-101.5-63.7c0-23.3 11.5-35 34.5-35s34.5 11.7 34.5 35c0 23-11.5 34.5-34.5 34.5s-34.5-11.5-34.5-34.5zM247.6 8C389.4 8 496 118.1 496 256c0 147.1-118.5 248-248.4 248C113.6 504 0 394.5 0 256 0 123.1 104.7 8 247.6 8zm.8 44.7C130.2 52.7 44.7 150.6 44.7 256c0 109.8 91.2 202.8 203.7 202.8 103.2 0 202.8-81.1 202.8-202.8.1-113.8-90.2-203.3-202.8-203.3z"/></svg> ]   

&lt;div style = "position: absolute;top: 0px;right: 0px;"&gt;&lt;img src="images/physalia.png" alt="The hex logo for plumbertableau package" width="500"&gt;&lt;/img&gt;&lt;/div&gt;

---

layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;Philip Leftwich - Physalia Courses&lt;/span&gt;&lt;/div&gt;

---
class: center, middle

.left-code[

* **Generalized** Linear Models

* Separating the mean from the error distribution

* Maximum Likelihood

* Modelling non-linear data

]

.right-plot[

&lt;img src="images/linear_regression_everywhere.jpg" title="Linear regression is everywhere" alt="Linear regression is everywhere" width="110%" /&gt;
]


---

## Recap OLS

&lt;img src="images/Sum of squares.png" title="OLS fits a line to produce the smallest amount of SSE" alt="OLS fits a line to produce the smallest amount of SSE" width="70%" /&gt;

---
## Recap OLS

$$
\LARGE{\hat{y_i} = a + bx}
$$
where:

`\(\hat{y_i}\)` = value estimated by the regression

`\(a\)` is the intercept (value of y when x = 0)

`\(b\)` is the slope of the regression line

`\(x\)` is the value of the explanatory variable


---

## Recap OLS

$$
\LARGE{y_i = a + bx+\epsilon}
$$

### Where:

`\(y_i\)` is the **observed** value of the response variable

`\(a\)` is the intercept (value of y when x = 0)

`\(b\)` is the slope of the regression line

`\(x\)` is the value of the explanatory variable

`\(\epsilon\)` is the value of the residual error

---

## OLS assumptions


.right-plot[
&lt;img src="images/ols_assumption.png" title="Outline of a hypothetical regression" alt="Outline of a hypothetical regression" width="100%" /&gt;
]

--

.left-code[

* Linear relationship

* Homogeneity of variance

* Normal distribution of residual variance

* Independent predictors


]

---

## Generalized Linear Models

A **Generalized** Linear Model is more flexible than a standard linear model because it allows us to separate the **error distribution** from the estimate of the **mean**

There are three components in a generalized linear model:

1. Linear Predictor – the equation of the slope and intercept

2. Link function – literally links the linear predictor to the mean (like an internal transformation)

3. Error distribution – so far we have always assumed Gaussian (Normal distribution) but can be others e.g Poisson, Binomial etc. 


---

## Generalized Linear Models

.pull-left[

**OLS Linear Model**

&lt;div class="figure"&gt;
&lt;img src="images/linear_equation.png" alt="Linear model equation includes mean estimate and error as a single equation" width="100%" /&gt;
&lt;p class="caption"&gt;Linear model equation includes mean estimate and error as a single equation&lt;/p&gt;
&lt;/div&gt;

]

--

.pull-right[

**Generalized Linear Model**

&lt;div class="figure"&gt;
&lt;img src="images/glm equation.png" alt="GLM models the estimate of the mean as a separate equation to estimates of the error distribution" width="100%" /&gt;
&lt;p class="caption"&gt;GLM models the estimate of the mean as a separate equation to estimates of the error distribution&lt;/p&gt;
&lt;/div&gt;

]

---





```r
lm(longevity ~ type + thorax, data = fruitfly) %&gt;% 
  broom::tidy()
```

```
## # A tibble: 4 x 5
##   term            estimate std.error statistic  p.value
##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       -56.5      11.1      -5.07 1.45e- 6
## 2 typeInseminated     3.45      2.76      1.25 2.14e- 1
## 3 typeVirgin        -13.3       2.76     -4.84 3.80e- 6
## 4 thorax            144.       13.1      11.0  6.41e-20
```



```r
glm(longevity ~ type + thorax, data = fruitfly, family= gaussian(link = "identity")) %&gt;% 
  broom::tidy()
```

```
## # A tibble: 4 x 5
##   term            estimate std.error statistic  p.value
##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       -56.5      11.1      -5.07 1.45e- 6
## 2 typeInseminated     3.45      2.76      1.25 2.14e- 1
## 3 typeVirgin        -13.3       2.76     -4.84 3.80e- 6
## 4 thorax            144.       13.1      11.0  6.41e-20
```



---

## What's the difference between transforming the data and using a link function?




```r
lsmodel1 &lt;- lm(yobsplus ~ x)
MASS::boxcox(lsmodel1)
```

&lt;img src="Generalized-Linear-Models_files/figure-html/unnamed-chunk-10-1.png" width="50%" /&gt;

---


```r
model1 &lt;- glm(yobsplus ~ x, family = gaussian(link = "log"))
broom::tidy(model1)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)     1.01     0.151      6.65 0.0000000250
## 2 x              -1.92     0.481     -4.00 0.000219
```



```r
model2 &lt;- glm(log(yobsplus) ~ x, family = gaussian(link = "identity"))
broom::tidy(model2)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)    0.554     0.350      1.58 0.120  
## 2 x             -2.05      0.604     -3.40 0.00138
```

---
.pull-left[



```r
model1 %&gt;% 
  check_model()
```

&lt;img src="Generalized-Linear-Models_files/figure-html/unnamed-chunk-13-1.png" width="110%" /&gt;

]

.pull-right[


```r
model2 %&gt;% 
  check_model()
```

&lt;img src="Generalized-Linear-Models_files/figure-html/unnamed-chunk-14-1.png" width="110%" /&gt;

]

---

## Maximum Likelihood

Maximum likelihood is a method that determined the values for the parameters of a model.

The values are chosen such that they *maximise* the *likelihood* that the process described by the model produced the observed data. 


$$
\LARGE{L(Y|\theta)}
$$

where:

`\(L\)` = Likelihood

`\(\theta\)` = unknown parameter

---

## Maximum Likelihood

As an example let's assume we are fitting our data using a model describing a Gaussian (normal) distribution.

$$
\LARGE{L(data;\mu,\sigma)}
$$
A Gaussian distribution has two parameters. The mean `\(\mu\)` and the standard deviation `\(\sigma\)`. Different values of these two parameters will result in different regression lines/fits to the data. 

They will also produce different *likelihood curves*. 

We want to know which combinations of mean and standard deviation produce the curve where we are *most likely* to observe all of the data points. 

---

## Maximum Likelihood

.right-plot[

&lt;img src="images/likelihood_curve.png" title="The 10 data points and possible Gaussian distributions from which the data were drawn. f1 is normally distributed with mean 10 and variance 2.25 (variance is equal to the square of the standard deviation), this is also denoted f1 ~ N (10, 2.25). f2 ~ N (10, 9), f3 ~ N (10, 0.25) and f4 ~ N (8, 2.25). The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data." alt="The 10 data points and possible Gaussian distributions from which the data were drawn. f1 is normally distributed with mean 10 and variance 2.25 (variance is equal to the square of the standard deviation), this is also denoted f1 ~ N (10, 2.25). f2 ~ N (10, 9), f3 ~ N (10, 0.25) and f4 ~ N (8, 2.25). The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data." width="100%" /&gt;
]

.left-code[

When the model can be explained by least squares, the outcome of the maximum likelihood will be identical. Both are designed to minimise error. 

Maximum likelihoods can solve a wider range of probability distributions, but are difficult to solve by hand in the same way as OLS, as they use these iterative methods. 
]
---

## GLM summary


```r
fly_glm &lt;- glm(longevity ~ type + thorax, data = fruitfly, family= gaussian(link = "identity")) 

summary(fly_glm)
```

```
## 
## Call:
## glm(formula = longevity ~ type + thorax, family = gaussian(link = "identity"), 
##     data = fruitfly)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -27.330   -6.803   -2.531    7.143   29.415  
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      -56.521     11.149  -5.069 1.45e-06 ***
## typeInseminated    3.450      2.759   1.250    0.214    
## typeVirgin       -13.349      2.756  -4.845 3.80e-06 ***
## thorax           143.638     13.064  10.995  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 125.7095)
## 
##     Null deviance: 38253  on 124  degrees of freedom
## Residual deviance: 15211  on 121  degrees of freedom
## AIC: 964.92
## 
## Number of Fisher Scoring iterations: 2
```

The GLM summary output, while mostly identical to the LM as the following differences:

* Deviance

* Dispersion, AIC &amp; Fisher Scoring 

---

## Deviance

A generalized linear model can be characterised in terms of two types of deviance:

1. The **null deviance**, which is a measure of the overall variability in the response variable:

`\(\Large{Dev_{null}=2(LnL_{saturated}-LnL_{null})}\)`

2. The **residual deviance**, which is a measure of the variability in the response variable that remains unexplained by the proposed model (equivalent to Sum of Square Errors):

`\(\Large{Dev_{residual}=2(LnL_{saturated}-LnL_{proposed})}\)`

In the previous example we had:

**Null deviance**: 38253 with df = 124

**Residual deviance**: 15211 with df = 121

---

## Chi square distribution

.right-plot[
&lt;img src="images/deviance_explained.png" title="The Saturated Model: is a model that assumes each data point has its own parameters, which means you have n parameters to estimate. The Proposed Model: assumes you can explain your data points with k parameters plus an intercept term, so you have k+1 parameters. The Null Model: assumes the exact “opposite”, it assumes one parameter for all of the data points, which means you only estimate 1 parameter." alt="The Saturated Model: is a model that assumes each data point has its own parameters, which means you have n parameters to estimate. The Proposed Model: assumes you can explain your data points with k parameters plus an intercept term, so you have k+1 parameters. The Null Model: assumes the exact “opposite”, it assumes one parameter for all of the data points, which means you only estimate 1 parameter." width="100%" /&gt;

]

.left-code[

**Null deviance**: 38253 with df = 124

**Residual deviance**: 15211 with df = 121

We can compare these values: 

`\(\chi^2=Null~deviance–Residual~deviance\)`

`\(\chi^2\)` = 38253-15211

`\(\chi^2\)` = 23042

With 3 predictors `\({\chi^2}_3=23042;~p&lt;0.001\)`

]
---

## Model simplification

Changes in deviance also follow the *Chi-square statistic* with *k* degrees of freedom.

$$
\chi^2= Proposed~model~deviance – Reduced~model~deviance
$$


.pull-left[

```r
drop1(fly_glm, test = "F")%&gt;% 
  as_tibble()
```

```
## # A tibble: 3 x 5
##      Df Deviance   AIC `F value`  `Pr(&gt;F)`
##   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1    NA   15211.  965.      NA   NA       
## 2     2   22756. 1011.      30.0  2.60e-11
## 3     1   30407. 1050.     121.   6.41e-20
```
]

.pull-right[


```r
drop1(fly_glm, test = "Chi") %&gt;% 
  as_tibble()
```

```
## # A tibble: 3 x 5
##      Df Deviance   AIC `scaled dev.` `Pr(&gt;Chi)`
##   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;
## 1    NA   15211.  965.          NA    NA       
## 2     2   22756. 1011.          50.4   1.16e-11
## 3     1   30407. 1050.          86.6   1.34e-20
```
]

The models are clearly the same, *but* F-tests for models where we estimate variance independently of the mean are a better choice for hypothesis testing. We will use chi-square distributions when using alternative error distributions e.g. Poisson &amp; Binomial.

---

## AIC: Akaike Information Criterion

A quantitative way to compare regression models.

**Unlike likelihood ratio test it CAN be used on non-nested models**

Takes into account how well the model predicts the data, while penalising it for increasing complexity

We want a descriptive model that balances prediction and complexiy, and is given by the model with the **lowest AIC value**

`\(\Large{AIC=2k-2Ln(L)}\)`

where:

`\(k\)` is the number of parameters

`\(L\)` is maximum likelihood

---

## Other outputs

###Dispersion

Indicates whether the distribution is wide or narrow

###Fisher score

A verbose output of how many iterations were required to resolve the model. A high number indicates the model may be having trouble converging (could be mis-specified)

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "dracula",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
